{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c6b8072-1da0-4232-bb58-82307ef59ab7",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This is a simple set of code to get you started, it contains some concepts (using Langchain for simplicity, but can also not using it):\n",
    "\n",
    "0. To start, you can either run this on your PC or with google colab (for free).\n",
    "\n",
    "If you are trying it on your own PC, please create a new directory and create a new environment.\n",
    "\n",
    "```\n",
    "cd my_project\n",
    "\n",
    "python -m venv .env\n",
    "\n",
    ".env\\Scripts\\activate\n",
    "\n",
    "pip install torch transformers pdfplumber langchain tiktoken faiss-cpu openai\n",
    "\n",
    "(.env) C:\\my_project\n",
    "```\n",
    "\n",
    "1. Loading data\n",
    "- We are using langchains PDFPlumberLoader to load the data. PDFPlumber is good at handling some specifica cases, and also able to extract text.\n",
    "- Langchain will split the PDF document into pages, and for each of the page the are metadata associated. This is useful for referencing.\n",
    "\n",
    "2. Split data into chuncks\n",
    "- We use the text_splitter to split the document into chunks of 1500 characters, with a overlab of 150 characters.\n",
    "- There are also other spliters, we just need to find the one which are suitable. The 150 overlap is to handle edge situations\n",
    "- You can see after the split the metadata are maintained for each chunck\n",
    "\n",
    "3. Load splitted text into a vectorstore, in this case we are using FAISS vectorstore\n",
    "- We are using OpenAIEmbeddings, in theory any embedding should work for queries.\n",
    "- Note that we are currently storing the DB in memory. We can also specific a location to persist the information processed on the machine.\n",
    "\n",
    "4. When the vectorstore is build, we can then test with some queries.\n",
    "- Example provide shows similarity search and max margine relavence search. The later may be better\n",
    "\n",
    "5. Select a large language model. So I have provided an example to use huggingface T5 as an example. It's not performing as good as openAI or LLAMA2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4f6de6-2e2c-4a47-90b4-53d5ee1e49af",
   "metadata": {},
   "source": [
    "## 1. Load the PDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4055f53e-57e0-4a7d-8cc9-c27115b28c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFPlumberLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91a7b265-78be-44b3-9889-a142185ab9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PDFPlumberLoader(\"./docs/1 Clinical paper_AI on AMD.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a03a512f-ea8f-475f-9a90-f5ef7d0abbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 pages loaded...\n"
     ]
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(f\"{len(docs)} pages loaded...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0dd20399-e67c-4719-be25-619d9b8699b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': './docs/1 Clinical paper_AI on AMD.pdf', 'file_path': './docs/1 Clinical paper_AI on AMD.pdf', 'page': 1, 'total_pages': 10, 'Creator': 'Arbortext Advanced Print Publisher 9.1.520/W Unicode', 'ModDate': \"D:20190726201857-07'00'\", 'CreationDate': \"D:20190724091338+05'30'\", 'Producer': 'Acrobat Distiller 9.0.0 (Windows); modified using iText 4.2.0 by 1T3XT', 'Subject': 'Clinical & Experimental Ophthalmology 0.0:null-null', 'WPS-PROCLEVEL': '3', 'WPS-JOURNALDOI': '10.1111/(ISSN)1442-9071', 'Title': 'Development and validation of a deep‐learning algorithm for the detection of neovascular age‐related macular degeneration from colour fundus photographs', 'WPS-ARTICLEDOI': '10.1111/ceo.13575'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cbffbb5-1c3f-4ad2-bd32-503f737a4d8a",
   "metadata": {},
   "source": [
    "## 2. Splitt Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8895b2f0-7cc8-44a2-a9d4-745e7f5746f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "chunk_size = 1500\n",
    "chunk_overlap = 150\n",
    "separators = [\"\\n\\n\", \" \", \"(?<=\\.) \", \"\", \"\\n\", \"(?<=, )\"]\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size = chunk_size, \n",
    "                                          chunk_overlap = chunk_overlap,\n",
    "                                          separators = separators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2eab4cce-6f9a-4770-89e6-905706d8df22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 chunks created\n"
     ]
    }
   ],
   "source": [
    "texts = text_splitter.split_documents(docs)\n",
    "print(f\"{len(texts)} chunks created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b6acc8-a7a3-4427-9edd-81bb901f6223",
   "metadata": {},
   "source": [
    "## 3. Load into vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87d8a0fb-b020-4023-83b9-a0312d2144ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "db = FAISS.from_documents(texts, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2aba783-e8dc-494d-82d2-9988a98862f7",
   "metadata": {},
   "source": [
    "## 4. Direct query the vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "89e2dfd8-645a-45a8-9258-6898bad6c9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What are the age of participants?\"\n",
    "sim_search_result = db.similarity_search(question) # this is based on the similrity (cosine distance)\n",
    "mmr_search_result = db.max_marginal_relevance_search(question) # MMR optimizes for similarity to query and diversity among selected documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e653eceb-9edd-44e5-96c5-693cf96866f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 4\n",
      "---------\n",
      "4 KEELETAL.\n",
      "FIGURE 2 Thedeepconvolutionalneuralnetworkusedinthisstudy.Datastreamisfromlefttoright.Afundusphotographisfirstlypre-\n",
      "processedbyscaling,subtractionoflocalspaceaveragecolour,downsizingtheim\n",
      "Page: 4\n",
      "---------\n",
      "86years (mean age 65.3 years, 60.1% female), 14% were DLA grading on three computers operating concurrently,\n",
      "born in Southern Europe (Greece, Italy or Malta), with the using a custom DLA software that\n",
      "Page: 6\n",
      "---------\n",
      "21777 participants from the MCCS. Of these, there were 0.967, 100% and 92.6%, respectively. This consisted of\n",
      "FIGURE 4 AMDtruepositive.ImagesA1,B1&C1showoriginalimageswithoutheat-map.A2showsheatmappre\n",
      "Page: 10\n",
      "---------\n",
      "10 KEELETAL.\n",
      "REFERENCES 16. AgurtoC,BarrigaES,MurrayV,etal.Automaticdetectionofdia-\n",
      "betic retinopathy and age-related macular degeneration in digital\n",
      "1. Wong WL, Su X, Li X, et al. Global prevalence o\n"
     ]
    }
   ],
   "source": [
    "for r in sim_search_result:\n",
    "    print(f\"Page: {r.metadata['page']}\")\n",
    "    print(\"-\".join([\"\" for x in range(10)]))\n",
    "    print(r.page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843fcf3d-4548-4e3c-84e7-9038072086af",
   "metadata": {},
   "source": [
    "## 5. Using A LLM for the Retrival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "61663021-8324-4107-a64b-3b7143db4dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, pipeline, AutoModelForSeq2SeqLM\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=model, \n",
    "    tokenizer=tokenizer, \n",
    "    max_length=100\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0b6f86d-6db2-4f67-aeb7-2a7b8601dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain_t5 = RetrievalQA.from_chain_type(\n",
    "    local_llm,\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "3f127fc6-6293-479d-8df8-e15d6e6e48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\") # OpenAI key\n",
    "chat = ChatOpenAI(temperature=0, openai_api_key=OPENAI_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3cf5345a-d5ec-4036-bb13-0419306aba84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "qa_chain_openai = RetrievalQA.from_chain_type(\n",
    "    chat,\n",
    "    retriever=db.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f1c0ac1a-6060-4b08-ae84-9a1cb7636e13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1210 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the age of participants?', 'result': '47'}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_t5(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ca8bbdb5-93b1-4e16-8f9c-5b3666172145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the age of participants?',\n",
       " 'result': 'The participants in the study were aged 47 to 86 years, with a mean age of 65.3 years.'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_chain_openai(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b05df-54bb-40a0-bf91-37ed40a7b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
